{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents: key concepts\n",
    "\n",
    "\n",
    "### Agent vs Chain\n",
    "\n",
    "The main difference between Agents and Chains is the ability to decide what to do next. Recall what we did in RAG notebook, per each user input we went through the same predefined list of steps.\n",
    "\n",
    "The Agents receives a task as input and makes a plan on how to solve it.\n",
    "Depending on implementation, it can either come up with whole sequence of steps at the beginning or decide on next steps on the fly.\n",
    "\n",
    "### Tools\n",
    "\n",
    "To enable the Agent to solve the given task, we can provide it with a list of available tools and allow it to decide which one to use.\n",
    "\n",
    "The example tools are: \n",
    "- web search <- maybe we need to fetch some data to answer the question\n",
    "- custom Python function <- run some predefined code\n",
    "- calculator <- LLMs are not good at more complex calculations\n",
    "\n",
    "\n",
    "### Agent taking next steps\n",
    "\n",
    "Remember that we are working with Language Models which can only predict the probability of next token. \n",
    "\n",
    "In order to automatically detecy whether the Agent wants to use a tool or take some next step we need to somehow structure its output to be able to parse it.\n",
    "\n",
    "To start with, we will use a mechanism implemented into `OpenAI` models called function calling. It allows us to define a set of functions and enables the model come up with sentence completion or function call.\n",
    "\n",
    "Since `OpenAI` is closed API we do not know how exactly it works, but good assuption would be that the base model is furher finetuned on a dataset with function calls \n",
    "where the output is probably in JSON to easily parse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/barteksadlej/Desktop/PROJECTS/workshops/LLM_intro/05.01.Agents.OpenAI-funcions.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barteksadlej/Desktop/PROJECTS/workshops/LLM_intro/05.01.Agents.OpenAI-funcions.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m+\u001b[39m y\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barteksadlej/Desktop/PROJECTS/workshops/LLM_intro/05.01.Agents.OpenAI-funcions.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m functions \u001b[39m=\u001b[39m [\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barteksadlej/Desktop/PROJECTS/workshops/LLM_intro/05.01.Agents.OpenAI-funcions.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barteksadlej/Desktop/PROJECTS/workshops/LLM_intro/05.01.Agents.OpenAI-funcions.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39madd_two_number\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barteksadlej/Desktop/PROJECTS/workshops/LLM_intro/05.01.Agents.OpenAI-funcions.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barteksadlej/Desktop/PROJECTS/workshops/LLM_intro/05.01.Agents.OpenAI-funcions.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     ]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/barteksadlej/Desktop/PROJECTS/workshops/LLM_intro/05.01.Agents.OpenAI-funcions.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m completion \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barteksadlej/Desktop/PROJECTS/workshops/LLM_intro/05.01.Agents.OpenAI-funcions.ipynb#W2sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpt-3.5-turbo-0613\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barteksadlej/Desktop/PROJECTS/workshops/LLM_intro/05.01.Agents.OpenAI-funcions.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     messages\u001b[39m=\u001b[39m[{\u001b[39m'\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mWhat is 1 + 1?\u001b[39m\u001b[39m'\u001b[39m}],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barteksadlej/Desktop/PROJECTS/workshops/LLM_intro/05.01.Agents.OpenAI-funcions.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     functions\u001b[39m=\u001b[39mfunctions,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barteksadlej/Desktop/PROJECTS/workshops/LLM_intro/05.01.Agents.OpenAI-funcions.ipynb#W2sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     function_call\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,  \u001b[39m# auto is default\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barteksadlej/Desktop/PROJECTS/workshops/LLM_intro/05.01.Agents.OpenAI-funcions.ipynb#W2sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39m# function_call={'name': 'add_two_number'}, # this will force the model to use the function\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barteksadlej/Desktop/PROJECTS/workshops/LLM_intro/05.01.Agents.OpenAI-funcions.ipynb#W2sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/PROJECTS/workshops/LLM_intro/env/lib/python3.10/site-packages/openai/_utils/_proxy.py:22\u001b[0m, in \u001b[0;36mLazyProxy.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, attr: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_proxied__(), attr)\n",
      "File \u001b[0;32m~/Desktop/PROJECTS/workshops/LLM_intro/env/lib/python3.10/site-packages/openai/_utils/_proxy.py:43\u001b[0m, in \u001b[0;36mLazyProxy.__get_proxied__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__get_proxied__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_cache:\n\u001b[0;32m---> 43\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load__()\n\u001b[1;32m     45\u001b[0m     proxied \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__proxied\n\u001b[1;32m     46\u001b[0m     \u001b[39mif\u001b[39;00m proxied \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/PROJECTS/workshops/LLM_intro/env/lib/python3.10/site-packages/openai/lib/_old_api.py:33\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__load__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m@override\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__load__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "def add_two_number(x, y):\n",
    "    \"\"\"Function that adds two numbers\"\"\"\n",
    "    return x + y\n",
    "\n",
    "functions = [\n",
    "        {\n",
    "            \"name\": \"add_two_number\",\n",
    "            \"description\": \"Add two numbers\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"x\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"first number\",\n",
    "                    },\n",
    "                    \"y\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"second number\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"x\", \"y\"],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo-0613',\n",
    "    messages=[{'role': 'user', 'content': 'What is 1 + 1?'}],\n",
    "    functions=functions,\n",
    "    function_call=\"auto\",  # auto is default\n",
    "    # function_call={'name': 'add_two_number'}, # this will force the model to use the function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model wants to call a function, the `finish_reason` is set to `function_call` instead of `stop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"index\": 0,\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": null,\n",
      "    \"function_call\": {\n",
      "      \"name\": \"add_two_number\",\n",
      "      \"arguments\": \"{\\n  \\\"x\\\": 1,\\n  \\\"y\\\": 1\\n}\"\n",
      "    }\n",
      "  },\n",
      "  \"finish_reason\": \"function_call\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "resp = completion['choices'][0] # type: ignore\n",
    "print(resp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason: function_call, f_name: add_two_number, f_args: {'x': 1, 'y': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type: ignore\n",
    "finish_reason = resp['finish_reason']\n",
    "f_name = completion['choices'][0]['message']['function_call']['name']\n",
    "f_args = json.loads(completion['choices'][0]['message']['function_call']['arguments'])\n",
    "\n",
    "print(f\"finish_reason: {finish_reason}, f_name: {f_name}, f_args: {f_args}\")\n",
    "\n",
    "f = globals()[f_name]\n",
    "f(**f_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order not to write function schema in JSON on our own, we can use utility function from LangChain to do it for us. The schema differs slightly from the one we wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import format_tool_to_openai_function\n",
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def add_two_number(x, y):\n",
    "    \"\"\"Function that adds two numbers\"\"\"\n",
    "    return x + y\n",
    "\n",
    "tools = format_tool_to_openai_function(add_two_number) # type: ignore\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"index\": 0,\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": null,\n",
      "    \"function_call\": {\n",
      "      \"name\": \"add_two_number\",\n",
      "      \"arguments\": \"{\\n  \\\"x\\\": 1,\\n  \\\"y\\\": 1\\n}\"\n",
      "    }\n",
      "  },\n",
      "  \"finish_reason\": \"function_call\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo-0613',\n",
    "    messages=[{'role': 'user', 'content': 'What is 1 + 1?'}],\n",
    "    functions=[tools],\n",
    "    function_call=\"auto\",\n",
    ")\n",
    "\n",
    "resp = completion['choices'][0] # type: ignore\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next notebook you will see how to automate the process of parsing the output and executing the function calls.\n",
    "\n",
    "For now, let just think how the Agents plan could look like.\n",
    "\n",
    "```\n",
    "user: \"What is 1 + 1?\"\n",
    "\n",
    "agent: (thinks) \"I will use calculator to solve this task\"\n",
    "\n",
    "agent: (uses tool) call add_two_number with arguments 1 and 1\n",
    "\n",
    "agent: (acts) provide the call result as an answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work?\n",
    "\n",
    "according to the [OpenAI docs](https://platform.openai.com/docs/guides/gpt/function-calling):\n",
    "\n",
    "---\n",
    "\n",
    "The latest models (`gpt-3.5-turbo-0613` and `gpt-4-0613`) have been fine-tuned to both detect when a function should to be called (depending on the input) and to respond with JSON that adheres to the function signature.\n",
    "\n",
    "---\n",
    "\n",
    "The most important takeaway is that the model is not actually executing the function, it is just generating the arguments in JSON format. It is up to us to parse the output and execute the function.\n",
    "\n",
    "Also, by providint the model with a list of function, we do not switch it to some magical mode, it is still a language model and it will generate text token by token.\n",
    "So it can still hallucinate and generate nonsense.\n",
    "\n",
    "Let's see what happens when we force it to produce a function call but provide some nonsense as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo-0613',\n",
    "    messages=[{'role': 'user', 'content': 'How to make one cake and then another?'}],\n",
    "    functions=functions,\n",
    "    function_call={'name': 'add_two_number'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"index\": 0,\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": null,\n",
      "    \"function_call\": {\n",
      "      \"name\": \"add_two_number\",\n",
      "      \"arguments\": \"{\\n  \\\"x\\\": 1,\\n  \\\"y\\\": 1\\n}\"\n",
      "    }\n",
      "  },\n",
      "  \"finish_reason\": \"stop\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "resp = completion['choices'][0] # type: ignore\n",
    "print(resp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "Now we know how to get structure action output from the model. What's missing is planing and exeuction.\n",
    "More complex question may require multiple actions to be executed and it's output may be used by the model to decide on next steps.\n",
    "\n",
    "Even for simple questions, we need to parse the output and execute the function call.\n",
    "Also it would be nice to get a nice final result sentence, not just a return value of the function.\n",
    "We will do it in next notebook.\n",
    "\n",
    "`OpenAI` function calling makes it easier to build agents because the API takes care of formatting the model prompt with available functions and gives us a nice JSON output.\n",
    "If we were to use some Open Source model we would need to implement this functionality on our own.\n",
    "\n",
    "Also note that Open Source models are usually trained on some plain text from the internet and only some of them are fine-tuned on e.g. code or other structured data. If you plan to use Open Source model probably the one fine-tuned on code would be the best choice to get structured output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
