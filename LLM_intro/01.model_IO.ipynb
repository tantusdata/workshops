{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model input and output\n",
    "\n",
    "<p><strong>To understand how input and output handling in LangChain works it is beneficial to first understand what format the model expects the input to be and what it outputs</strong></p>\n",
    "\n",
    "<p>Under the hood LLM takes a string input and produces a string output.\n",
    "But for various models the input format may differ depending on how the training data was formatted.\n",
    "</p>\n",
    "\n",
    "<p>OpenAI API offers two endpoints. ChatCompletion and Completion, the second one will be depreciated on January 04, 2024 but lets compare them to see and understand the difference and how LangChain helps as a high level wrapper around them. \n",
    "Completion. </p>\n",
    "\n",
    "<p>For the Completion API the input is left unhanged, the same as we write it. The call to the API with their python library looks like that:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-86cZ5euhXwVDrZ0xC6wDtYTXYOhsb\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1696588903,\n",
      "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\nWhy did the bear refuse to wear shoes?\\n\\nBecause he had bear feet!\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 18,\n",
      "    \"completion_tokens\": 16,\n",
      "    \"total_tokens\": 34\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "behaviour = \"an expert at telling jokes\"\n",
    "topic = \"bear\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=f\"\"\"\\\n",
    "You are {behaviour}.\n",
    "User question: Tell me a joke about {topic}.\"\"\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(response)\n",
    "# read more: https://platform.openai.com/docs/guides/gpt/completions-api"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As we can see the response is a json object and to extract the generated text we have to write </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the bear refuse to wear shoes?\\n\\nBecause he had bear feet!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['choices'][0]['text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>For the ChatCompletion API, the input consist of a list of messages which represent current chat history and may also include a system message which helps set the behavior of the assistant.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-86cZ6hMirsiNCEheLF37szPSAQXFa\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1696588904,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Sure, here's a bear-themed joke for you:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they have bear feet!\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 58,\n",
      "    \"completion_tokens\": 24,\n",
      "    \"total_tokens\": 82\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"You are {behaviour}.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hi. What's your name?\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"Hello. I am {behaviour}. What can I do for you today?\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Tell me a joke about {topic}\"}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The difference comes from the fact that chat modes are trained with specifically formatted input. While we don't know the format for OpenAI models, lets see for example format for open source Meta Lama2:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "System_Message_Here\n",
      "<</SYS>>\n",
      "\n",
      "User_Msg_1 [/INST] Assistant_Msg_1 [INST] User_Msg_2 [/INST] Assistant_Msg_2 [INST] User_Msg_3 [/INST]\n"
     ]
    }
   ],
   "source": [
    "print('[INST] <<SYS>>\\nSystem_Message_Here\\n<</SYS>>\\n\\nUser_Msg_1 [/INST] Assistant_Msg_1 [INST] User_Msg_2 [/INST] Assistant_Msg_2 [INST] User_Msg_3 [/INST]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How LangChain can help?\n",
    "\n",
    "<p>When building more complex applications one don't want to focus on parsing json or constantly checking for API specification changes from different providers.\n",
    "\n",
    "Here comes LangChain which removes the burden of doing it on one's own.  \n",
    "It provides an abstraction for working with language models, along with implementation for most popular providers. </p>\n",
    "\n",
    "<p>Let's see how the above examples can be implemented in it.\n",
    "\n",
    "Note that for prompt we don't use python f-string</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n\\nWhy did the bear refuse to wear shoes?\\n\\nBecause he had bear feet!',\n",
       " AIMessage(content=\"Sure, here's a bear-themed joke for you:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they have bear feet!\"))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI # for ChatCompletion API\n",
    "from langchain.llms import OpenAI # for Completion API\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate, \n",
    "    ChatPromptTemplate, \n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    ")\n",
    "from langchain.prompts.chat import HumanMessage\n",
    "\n",
    "completion_prompt = PromptTemplate(\n",
    "    template=\"\"\"\\\n",
    "You are {behaviour}.\n",
    "User question: Tell me a joke about {topic}.\"\"\",\n",
    "    input_variables=['behaviour', 'topic'])\n",
    "\n",
    "completion_api = OpenAI(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0.0,)\n",
    "\n",
    "completion_resp = completion_api(completion_prompt.format(behaviour=behaviour, topic=topic))\n",
    "\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"You are {behaviour}.\"),\n",
    "        HumanMessage(content=\"Hi. What's your name?\"),\n",
    "        AIMessagePromptTemplate.from_template(\"Hello. I am {behaviour}. What can I do for you today?\"),\n",
    "    ]\n",
    "    # prompts support addition\n",
    ") + HumanMessagePromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "chat_api = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0)\n",
    "\n",
    "chat_resp = chat_api(chat_prompt.format_messages(behaviour=behaviour, topic=topic))\n",
    "\n",
    "completion_resp, chat_resp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Under the hood ChatOpenAI creates the desired list of dicts with correct format for us, while OpenAI class expects only input string. We can inspect the formats to check how they look like:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an expert at telling jokes.\\nUser question: Tell me a joke about bear.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_prompt.format(behaviour=behaviour, topic=topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are an expert at telling jokes.'),\n",
       " HumanMessage(content=\"Hi. What's your name?\"),\n",
       " AIMessage(content='Hello. I am an expert at telling jokes. What can I do for you today?'),\n",
       " HumanMessage(content='Tell me a joke about bear')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.format_messages(behaviour=behaviour, topic=topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are an expert at telling jokes.'},\n",
       " {'role': 'user', 'content': \"Hi. What's your name?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Hello. I am an expert at telling jokes. What can I do for you today?'},\n",
       " {'role': 'user', 'content': 'Tell me a joke about bear'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_api._create_message_dicts(chat_prompt.format_messages(behaviour=behaviour, topic=topic), stop=None)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change provider\n",
    "\n",
    "If you want to change LLM provider, LangChain makes it very easy.\n",
    "For example, let's say you want to test your app with Anthropic:\n",
    "The only thing you have to do is to hange the model, templates stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human: <admin>You are an expert at telling jokes.</admin>\n",
      "\n",
      "Human: Hi. What's your name?\n",
      "\n",
      "Assistant: Hello. I am an expert at telling jokes. What can I do for you today?\n",
      "\n",
      "Human: Tell me a joke about bear\n",
      "\n",
      "Assistant:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "import os\n",
    "# this must be set to instantiate the class\n",
    "os.environ['ANTHROPIC_API_KEY'] = 'dummy key'\n",
    "print(ChatAnthropic()._convert_messages_to_prompt(chat_prompt.format_messages(behaviour=behaviour, topic=topic)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
