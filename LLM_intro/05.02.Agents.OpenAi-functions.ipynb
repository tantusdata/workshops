{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents in practice\n",
    "\n",
    "In this notebook we will first see how the agent works in practice.\n",
    "LangChain comes with a set of predefined agents that can be used with tools of our choice.\n",
    "\n",
    "We will start with `AgentType.OPENAI_FUNCTIONS` - it runs as long as model's output `finish_reason = function_call` and includes prevoius function results into the next round prompt. One disadvantage of such approach is that it doesn't plan ahead - it only decides on (calling a function / finishing with final sentence) based on the previous function results. \n",
    "\n",
    "Next step will be to overcome those pitfalls and implement it from scratch to see how it works.\n",
    "It will give you a thorough understanding of current possibilities but also limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More tools\n",
    "\n",
    "To make it more interesting, apart from using our simple calculator function to add two numbers, we will provide the model with web search to enable it to fetch missing information.\n",
    "\n",
    "One caveat is that the web search does not always return results in nice format, usually it is a concatenation of many different information sources. \n",
    "\n",
    "We will use `GoogleSerperAPIWrapper` which is a wraper for Google search and gives much better results than free `DuckDuckGoSearchRun`.\n",
    "One additional setup step here is to create free account with 2500 free requests. In only requires confirming the email.\n",
    "Go to https://serper.dev , sing up, confirm email and copy the API key.\n",
    "Place in in .env file with name `SERPER_API_KEY` or enter it directly in the cell belowe.\n",
    "We will modify it slightly to reduce results length to just include first 250 characters. Longer results will only distract the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'44Â°F'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import getpass\n",
    "\n",
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.tools import GoogleSerperResults\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "if os.getenv(\"SERPER_API_KEY\") is None:\n",
    "    os.environ[\"SERPER_API_KEY\"] = getpass.getpass(\"Serper API Key not set! Enter Serper API Key: \")\n",
    "\n",
    "\n",
    "search = GoogleSerperAPIWrapper()\n",
    "\n",
    "def search_run(input):\n",
    "    return search.run(input)[:250]\n",
    "\n",
    "search_run(\"What is the current weather in Vilnius?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important think to note when testing queries: ask about something which chat could not be trained on, meaning before September 2021. Otherwise it might be able to answer without any search.\n",
    "Also the results in this notebook will for sure vary each rerun depending on the search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import tool, Tool\n",
    "from langchain.tools import format_tool_to_openai_function\n",
    "\n",
    "@tool\n",
    "def add_two_number(x, y):\n",
    "    \"\"\"Function that adds two numbers\"\"\"\n",
    "    return x + y\n",
    "\n",
    "@tool \n",
    "def multiply_two_number(x, y):\n",
    "    \"\"\"Function that multiplies two numbers\"\"\"\n",
    "    return x * y\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"web_search\", # note that we can't use spaces with OpenAi function names!\n",
    "        func=search_run,\n",
    "        description=\"Usefull if you need to find out some additional information. You should ask targeted questions.\"\n",
    "    ),\n",
    "    add_two_number,\n",
    "    multiply_two_number\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools, # type: ignore\n",
    "    llm=ChatOpenAI(model='gpt-3.5-turbo-1106', temperature=0.),\n",
    "    agent=AgentType.OPENAI_FUNCTIONS, \n",
    "    verbose=True,\n",
    "    max_iter=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `population of France in 2023`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe current population of France in 2023 is 64,756,584, a 0.2% increase from 2022. The population of France in 2022 was 64,626,628, a 0.15% increase from 2021. The population of France in 2021 was 64,531,444, a 0.08% increase from 2020.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `population of Germany in 2023`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mGermany's population in 2023 Germany's total population was 83.31 million in January 2023. Data shows that Germany's population decreased by 114 thousand (-0.1 percent) between 2022 and 2023. 50.7 percent of Germany's population is female, while 49.3\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `add_two_number` with `{'x': 64756584, 'y': 83310000}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m148066584\u001b[0m\u001b[32;1m\u001b[1;3mThe sum of the populations of France and Germany as of 2023 is 148,066,584.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent.invoke(\"What is the sum of populations of France and Germany as of 2023?\") # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the sum of populations of France and Germany as of 2023?',\n",
       " 'output': 'The sum of the populations of France and Germany as of 2023 is 148,066,584.'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the agent worked fine but search about France returned `exceeding 68 million` but chat went with some exact number `68432200`\n",
    "\n",
    "Let's check some other query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `price of 123 Tesla stocks`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mTesla Inc stocks price quote with latest real-time prices, charts, financials, latest news, technical analysis and opinions. Tesla Share Price Live Today:Get the Live stock price of TSLA Inc., and quote, performance, latest news to help you with stoc\u001b[0m\u001b[32;1m\u001b[1;3mThe current price of Tesla stocks is not provided in the search results. I recommend checking a financial news website or a stock market app for the most up-to-date information on the price of Tesla stocks.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent.invoke(\"What is the price of 123 Tesla stocks?\") # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the price of 123 Tesla stocks?',\n",
       " 'output': 'The current price of Tesla stocks is not provided in the search results. I recommend checking a financial news website or a stock market app for the most up-to-date information on the price of Tesla stocks.'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model didn't came up with an idea to first check the price for one stock and then multiply it appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `current price of Tesla stock`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m222.11 -0.07 (0.03%)\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search` with `current price of Microsoft stock`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m363.20 +2.67 (0.74%)\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `add_two_number` with `{'x': 2, 'y': 3}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m5\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `multiply_two_number` with `{'x': 222.11, 'y': 2}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m444.22\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `multiply_two_number` with `{'x': 363.2, 'y': 3}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m1089.6\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `add_two_number` with `{'x': 444.22, 'y': 1089.6}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m1533.82\u001b[0m\u001b[32;1m\u001b[1;3mThe price of two Tesla stocks is $444.22, and the price of three Microsoft stocks is $1089.60. Adding these together, we get a total of $1533.82.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent.invoke(\"What is the price of two Tesla stocks plus the price of three Microsoft stocks?\") # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one went much better!\n",
    "When I run it locally it went with one additional unnecessary of `Invoking: `add_two_number` with `{'x': 2, 'y': 3}` but it may vary depending on the search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the agent\n",
    "\n",
    "Now we will build the agent from scratch. Instead of using function calling directly thourgh `OpenAi API`, we will prompt the model (and hope it comply :) ) to produce valid function arguments as output.\n",
    "\n",
    "When using function calling, the only output we get when the model decides to call a functions are its arguments. We do not get any explanation why it decided to call it.\n",
    "To make it better, we can include additional `Think` step before `Act` as suggested in [ReAct paper](https://react-lm.github.io). \n",
    "It gives the LLM a chance to explain its decision and as a result leads to better outcomes.\n",
    "\n",
    "#### Providing actions results to model\n",
    "\n",
    "The model prompt will follow this pattern:\n",
    "```\n",
    "general instructions\n",
    "\n",
    "QUESTION\n",
    "\n",
    "steps (possibly many):\n",
    "Thought: I need to fetch some information about X from the internet\n",
    "Action: web_search\n",
    "Action Input: \"What is X\"\n",
    "Observation: \"X is ...\"\n",
    "\n",
    "and at the end:\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "``` \n",
    "\n",
    "The `observation` will be the output of the action.\n",
    "In each step, the model will generate Thought, Action and Action Input (or Final Answer if it is the end) and we will run it localy and provide it with the result.\n",
    "\n",
    "To do so we can take advantage of optionally stoping the LLM from further generation if it produces a specific string, in this case `Observation: `.\n",
    "Later we will extend the prompt with action results and run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import BaseChatPromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage, BaseMessage\n",
    "import re\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make thinks simpler, we won't use JSON for forcing the model to produce structured output.\n",
    "The `OpenAI` models are powerful enought to spot the pattern int the prompt and follow it, of course to some extent.\n",
    "\n",
    "And because of that, the action results will be provided as a string, not a JSON object. We need to change the tools to work with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add_numbers(input: str):\n",
    "    \"\"\"Usefull if you need to add numbers e.g 2+3. Input should be in the format `x+y+...`\"\"\"\n",
    "    return sum([float(x) for x in input.split(\"+\")])\n",
    "\n",
    "@tool \n",
    "def multiply_numbers(input: str):\n",
    "    \"\"\"Usefull if you need to multiply numbers e.g 2*3. Input should be in the format `x*y*...`\"\"\"\n",
    "    return reduce(lambda x, y: x * y, [float(x) for x in input.split(\"*\")])\n",
    "\n",
    "tools2 = [\n",
    "    Tool(\n",
    "        name=\"web_search\",\n",
    "        func=search_run,\n",
    "        description=\"Usefull if you need to find out some additional information. You should ask targeted questions.\"\n",
    "    ),\n",
    "    add_numbers,\n",
    "    multiply_numbers\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent prompt template\n",
    "\n",
    "There are many prompt tempales for agent. We will work with the simple short one.\n",
    "If you want to search for more complex prompts the [LangChain Hub](https://smith.langchain.com/hub?organizationId=8a005f7e-7bfc-5a51-856d-a98ed4e65f71) has many of them, some follow JSON format some don't.\n",
    "\n",
    "We also need to write custom prompt formatting to meed our defined structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the base template\n",
    "template = \"\"\"Complete the objective as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format. When you know the answer, write it as `Final Answer: `\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "class CustomPromptTemplate(BaseChatPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "\n",
    "    def format_messages(self, **kwargs) -> List[BaseMessage]:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        formatted = self.template.format(**kwargs)\n",
    "        return [HumanMessage(content=formatted)]\n",
    "    \n",
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools=tools2,\n",
    "    input_variables=[\"input\", \"intermediate_steps\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `input_variables=[\"input\", \"intermediate_steps\"]` might look confusing at first, because we only specify part of whats is in the prompt.\n",
    "\n",
    "`input` and `intermediate_steps` will be passed to `format_messages` as `kwargs` everything else will be set in this function. Later `pop` will remove `intermediate_steps` from kwargs and in the following lines we will add what is missing in the template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing LLM output\n",
    "\n",
    "If the models follows the patters for thought, action, observation etc. at each step we will either get the output\n",
    "```\n",
    "Thought: ...\n",
    "Action: ...\n",
    "Action Input: ...\n",
    "Observation:| <- here ends the model output because we will use stop criteria\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "Thought: I now know the final answer\n",
    "Final Answer: ...\n",
    "```\n",
    "\n",
    "So we need to write a custom parser to detect those two situations and extract the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # According to LangChain docs:\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\n",
    "                    \"output\": llm_output.split(\"Final Answer:\")[-1].strip() # <- here we take what is after the final answer marker\n",
    "                },\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        # this regular expression matches Action: and Action Input: lines\n",
    "        # If you don't fully understand it, don't worry, it's not that important, usually you can google/chat-gpt what you need\n",
    "        # If you want to get a better feeling about it, you can explore it here: https://regex101.com/r/dTDhMy/1\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "    \n",
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the agent\n",
    "\n",
    "Now we have all necessary building blocks to create our agent.\n",
    "We will first use predefined class `LLMSingleActionAgent`. Single coresponds to fact that it can only produce one action at a time, not a sequence of actions.\n",
    "\n",
    "We set `stop=[\"\\nObservation:\"]` which will be sent with prompt to `OpenAI API` and will stop the generation when this string is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo-1106', temperature=0)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "tool_names = [tool.name for tool in tools2]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"],\n",
    "    # allowed_tools=tool_names\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find the populations of France and Germany and then add them together.\n",
      "Action: web_search\n",
      "Action Input: \"population of France 2023\"\u001b[0m\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3mFYI : At 1er January 2023, France counts 68.043 million inhabitants, an increase of 0.3% over one year, limited by a fall in birth rates and a high mortality rate, Insee said. Of these, 65.8 million reside in mainland France and 2.2 million in the fi\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find the population of Germany as well.\n",
      "Action: web_search\n",
      "Action Input: \"population of Germany 2023\"\u001b[0m\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3mGermany's total population was 83.31 million in January 2023. Data shows that Germany's population decreased by 114 thousand (-0.1 percent) between 2022 and 2023. 50.7 percent of Germany's population is female, while 49.3 percent of the population is\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mNow I can add the populations together.\n",
      "Action: add_numbers\n",
      "Action Input: 68.043+83.31\u001b[0m\n",
      "\n",
      "Observation:\u001b[33;1m\u001b[1;3m151.353\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The sum of the populations of France and Germany as of 2023 is 151.353 million.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sum of the populations of France and Germany as of 2023 is 151.353 million.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"What is the sum of populations of France and Germany as of 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should use web_search to find the current price of Tesla stocks.\n",
      "Action: web_search\n",
      "Action Input: \"current price of Tesla stocks\"\u001b[0m\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3m222.11 -0.07 (0.03%)\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to multiply the current price by the number of stocks.\n",
      "Action: multiply_numbers\n",
      "Action Input: 123*222.11\u001b[0m\n",
      "\n",
      "Observation:\u001b[38;5;200m\u001b[1;3m27319.530000000002\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The price of 123 Tesla stocks is $27,319.53\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The price of 123 Tesla stocks is $27,319.53'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"What is the price of 123 Tesla stocks?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts and observations\n",
    "\n",
    "As you can see, adding `Thought` step makes model predictions much better.\n",
    "It now knows that it should check the price of one stock and then multiply it by the number of stocks.\n",
    "\n",
    "There are many [tools](https://python.langchain.com/docs/integrations/tools) and [toolkits](https://python.langchain.com/docs/integrations/toolkits) implemented in LangChain which you can check out but the idea is the same as whit simple web search or Python function call.\n",
    "You must provide the model with its names and description of what it does and how to use it.\n",
    "For example:\n",
    "- `WikipediaQueryRun` <- gets the top matching wikipedia article, but becareful, it fetches the whole article which can be very long and fill up the prompt quickly.\n",
    "\n",
    "Quick tips to improve the agent:\n",
    "- add current date to the prompt at runtime\n",
    "- add user location to the prompt at runtime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
