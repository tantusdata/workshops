{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Chains\n",
    "\n",
    "In this notebook we will go through the most popular usecase for LangChain which is building Chains.\n",
    "\n",
    "### What is a Chain?\n",
    "\n",
    "Chain consists of chardcoded list of steps, which one can later execute on custom input. \n",
    "Recap what we did in the previous notebook, first we formatted the prompt and then we send it to the model for processing.\n",
    "This is the simples example of a chain. Later we will explore more complex ones.\n",
    "\n",
    "\n",
    "### LangChain Expression Language (LCEL)\n",
    "\n",
    "LCEL is a declarative way to easily write and compose custom chains.\n",
    "Apart from enabling developers to do it in a very pythonic way, it has several other benefits.\n",
    "Any Chain constructed this way will automatically have full sync, async, batch, and streaming support. Also any components that can be run in parallel automatically are.\n",
    "\n",
    "Let's start with rewriting the chain from previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    ")\n",
    "from langchain.prompts.chat import HumanMessage\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a bear-themed joke for you:\n",
      "\n",
      "Why don't bears wear shoes?\n",
      "\n",
      "Because they have bear feet!\n"
     ]
    }
   ],
   "source": [
    "behaviour = \"an expert at telling jokes\"\n",
    "topic = \"bear\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"You are {behaviour}.\"),\n",
    "        HumanMessage(content=\"Hi. What's your name?\"),\n",
    "        AIMessagePromptTemplate.from_template(\"Hello. I am {behaviour}. What can I do for you today?\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_api = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0)\n",
    "\n",
    "chat_chain = chat_prompt | chat_api | StrOutputParser()\n",
    "\n",
    "res = chat_chain.invoke({\"behaviour\": behaviour, \"topic\": topic})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'behaviour': {'title': 'Behaviour'},\n",
       "  'topic': {'title': 'Topic'}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'StrOutputParserOutput', 'type': 'string'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.output_schema.schema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks\n",
    "\n",
    "As you can see, the code is cleaner and easy to understand.\n",
    "\n",
    "Now let's dig deeper and understand how those building blocks can be chained together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import (\n",
    "    RunnableLambda, \n",
    "    RunnableMap, \n",
    "    RunnableBranch,\n",
    "    RunnablePassthrough\n",
    ")\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from operator import itemgetter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableLambda` is a wrapper arounf arbitrary function which makes it compatible with Expression Language.\n",
    "\n",
    "It is important to remember, that the function used in chain must accept only one argument. \n",
    "So if you have a function which takes more than one argument, you can refactor it, to take a dictionary of inputs as argument or call it from another function which does so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableLambda(lambda x: x + 1) | (lambda x: x * 2) # this syntax also works\n",
    "chain.invoke(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 2 3 4'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_custom_fn(a, b, c, d):\n",
    "    return f\"{a} {b} {c} {d}\"\n",
    "\n",
    "def fn_wrapper(inputs):\n",
    "    a = inputs[\"a\"]\n",
    "    b = inputs[\"b\"]\n",
    "    c = inputs[\"c\"]\n",
    "    d = inputs[\"d\"]\n",
    "    return my_custom_fn(a, b, c, d)\n",
    "\n",
    "# or using the inspect module\n",
    "from inspect import signature\n",
    "def fn_wrapper2(inputs):\n",
    "    fn_sig = signature(my_custom_fn)\n",
    "    return my_custom_fn(**{k: inputs[k] for k in fn_sig.parameters.keys()})\n",
    "\n",
    "chain = RunnableLambda(fn_wrapper)\n",
    "chain.invoke({\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableMap` can execute multiple Runnables in parallel and return the output of these Runnables as a map.\n",
    "\n",
    "Each element of the map will get a full copy of the input and all the outputs will get combined into single dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 inputs: [1, 2, 3]\n",
      "f2 inputs: [1, 2, 3]\n",
      "f3 inputs: [1, 2, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1 return value': 'f1',\n",
       " 'f2 return value': ['f2', 'f2'],\n",
       " 'f3 return value': {'some_name': 'f3'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1(inputs):\n",
    "    print(f\"f1 inputs: {inputs}\")\n",
    "    return \"f1\"\n",
    "\n",
    "def f2(inputs):\n",
    "    print(f\"f2 inputs: {inputs}\")\n",
    "    return [\"f2\", \"f2\"]\n",
    "\n",
    "def f3(inputs):\n",
    "    print(f\"f3 inputs: {inputs}\")\n",
    "    return {\"some_name\": \"f3\"}\n",
    "\n",
    "chain = RunnableMap({\n",
    "    \"f1 return value\": f1,\n",
    "    \"f2 return value\": f2,\n",
    "    \"f3 return value\": f3\n",
    "})\n",
    "chain.invoke([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 inputs: [1, 2, 3]\n",
      "f2 inputs: [1, 2, 3]\n",
      "f3 inputs: [1, 2, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'copy nr. 1': {'f1 return value': 'f1',\n",
       "  'f2 return value': ['f2', 'f2'],\n",
       "  'f3 return value': {'some_name': 'f3'}},\n",
       " 'copy nr. 2': {'f1 return value': 'f1',\n",
       "  'f2 return value': ['f2', 'f2'],\n",
       "  'f3 return value': {'some_name': 'f3'}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2 = chain | {\n",
    "    \"copy nr. 1\": lambda x: x,\n",
    "    \"copy nr. 2\": lambda x: x,\n",
    "}\n",
    "\n",
    "chain2.invoke([1, 2, 3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `itemgetter` to extract arguments from input dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 2, 'f2': 'hello world'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1(x):\n",
    "    return x + 1\n",
    "\n",
    "def f2(y):\n",
    "    return y + \" world\"\n",
    "\n",
    "chain = RunnableMap({\n",
    "    \"f1\": itemgetter(\"x\") | RunnableLambda(f1),\n",
    "    \"f2\": itemgetter(\"y\") | RunnableLambda(f2),\n",
    "})\n",
    "\n",
    "chain.invoke({\"x\": 1, \"y\": \"hello\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableMap` runs in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0066568851470947\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from operator import itemgetter\n",
    "\n",
    "sleep_chain = RunnableMap({\n",
    "    't1': itemgetter('t1') | RunnableLambda(time.sleep), \n",
    "    't2': itemgetter('t2') | RunnableLambda(time.sleep),\n",
    "    't3': itemgetter('t3') | RunnableLambda(time.sleep),\n",
    "})\n",
    "\n",
    "start = time.time()\n",
    "sleep_chain.invoke({'t1': 3, 't2': 3, 't3': 3})\n",
    "end = time.time()   \n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnablePassthrough` passes the whole input further.\n",
    "\n",
    "It can be used to wrap a input into dictionary to provide keys necessary for some function, or make the input available in later chain steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"results for 'I am looking for ...': search results\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search(query):\n",
    "    return \"search results\"\n",
    "\n",
    "def combine_results(results):\n",
    "    query = results[\"query\"]\n",
    "    search_results = results[\"search_results\"]\n",
    "\n",
    "    return f\"results for '{query}': {search_results}\"\n",
    "\n",
    "chain = RunnableMap({\n",
    "    \"query\": RunnablePassthrough(),\n",
    "    \"search_results\": RunnableLambda(search),\n",
    "}) | RunnableLambda(combine_results)\n",
    "\n",
    "chain.invoke(\"I am looking for ...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableBranch` is used for conditional branching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handling int\n",
      "handling str\n",
      "handling default\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 'hello world', 1.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def handle_int(x):\n",
    "    print(\"handling int\")\n",
    "    return x + 1\n",
    "\n",
    "def handle_str(x):\n",
    "    print(\"handling str\")\n",
    "    return x + \" world\"\n",
    "\n",
    "def default_handler(x):\n",
    "    print(\"handling default\")\n",
    "    return x\n",
    "\n",
    "chain = RunnableBranch(\n",
    "    (lambda x: isinstance(x, int), RunnableLambda(handle_int)),\n",
    "    (lambda x: isinstance(x, str), RunnableLambda(handle_str)),\n",
    "    default_handler\n",
    ")\n",
    "\n",
    "# or passing a function that handles routing\n",
    "def route(inputs):\n",
    "    if isinstance(inputs, int):\n",
    "        return handle_int(inputs)\n",
    "    elif isinstance(inputs, str):\n",
    "        return handle_str(inputs)\n",
    "    else:\n",
    "        return default_handler(inputs)\n",
    "    \n",
    "chain2 = RunnableLambda(route)\n",
    "\n",
    "chain.invoke(1), chain.invoke(\"hello\"), chain2.invoke(1.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "##### Create a chain for chatting with the model and storing conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'property' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m MessagesPlaceholder\u001b[39m.\u001b[39;49minput_variables()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'property' object is not callable"
     ]
    }
   ],
   "source": [
    "MessagesPlaceholder(variable_name=\"history\").in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful chatbot.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "])\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"history\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "def get_question_and_history(input_string: str) -> Dict[str, str]:\n",
    "\n",
    "    return {\n",
    "        \"question\": ...,\n",
    "        \"history\": memory.load_memory_variables(inputs=None) # get list of history messages from memory\n",
    "    }\n",
    "\n",
    "def get_answer_and_pass_question_through(input_dict: Dict[str, str]) -> Dict[str, str]:\n",
    "\n",
    "    get_answer_chain = ... # format prompt > call model > extract output from AI Message object\n",
    "\n",
    "    return {\n",
    "        \"answer\": get_answer_chain.invoke(...), # recall from earlier, that prompt accepts a dictionary with key names == template variables\n",
    "        \"question\": ..., # pass question further \n",
    "    }\n",
    "\n",
    "def update_history_and_return_answer(inputs: Dict[str, str]) -> str:\n",
    "    # we use question/answer keys because they are required by the object, but we dont use it here\n",
    "    memory.save_context({'question': ...}, {'answer': ...}) # store both question and answer in memory object, \n",
    "    \n",
    "    return ... # return answer string\n",
    "\n",
    "chat_chain = RunnableLambda(get_question_and_history) | RunnableLambda(get_answer_and_pass_question_through) | RunnableLambda(update_history_and_return_answer) | StrOutputParser()\n",
    "\n",
    "\n",
    "print(chat_chain.invoke(\"Hi. What's your name?\"))\n",
    "print(chat_chain.invoke(\"What is 2+2?\"))\n",
    "\n",
    "# see list of history messages\n",
    "print(...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am a chatbot, so I don't have a personal name. You can simply refer to me as \"Chatbot\" or any other name you prefer. How can I assist you today?\n",
      "The sum of 2 and 2 is 4.\n",
      "[HumanMessage(content=\"Hi. What's your name?\"), AIMessage(content='Hello! I am a chatbot, so I don\\'t have a personal name. You can simply refer to me as \"Chatbot\" or any other name you prefer. How can I assist you today?'), HumanMessage(content='What is 2+2?'), AIMessage(content='The sum of 2 and 2 is 4.')]\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful chatbot.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "])\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"history\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "\n",
    "def get_question_and_history(input_str: str) -> Dict[str, str]:\n",
    "    return {\n",
    "        \"question\": input_str,\n",
    "        \"history\": memory.load_memory_variables(inputs=None)['history'],\n",
    "    }\n",
    "\n",
    "def get_answer_and_pass_question_through(input_dict: Dict[str, str]) -> Dict[str, str]:\n",
    "    get_answer_chain = prompt | model | StrOutputParser()\n",
    "    return {\n",
    "        \"question\": input_dict['question'],\n",
    "        \"answer\": get_answer_chain.invoke(input_dict),\n",
    "    }\n",
    "\n",
    "def update_history_and_return_answer(inputs: Dict[str, str]) -> str:\n",
    "    memory.save_context({'question': inputs['question']}, {'answer': inputs['answer']})\n",
    "\n",
    "    return inputs['answer']\n",
    "\n",
    "chat_chain = RunnableLambda(get_question_and_history) | RunnableLambda(get_answer_and_pass_question_through) | RunnableLambda(update_history_and_return_answer)\n",
    "\n",
    "print(chat_chain.invoke(\"Hi. What's your name?\"))\n",
    "print(chat_chain.invoke(\"What is 2+2?\"))\n",
    "\n",
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
