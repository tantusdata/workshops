{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Chains\n",
    "\n",
    "In this notebook we will go through the most popular usecase for LangChain which is building Chains.\n",
    "\n",
    "### What is a Chain?\n",
    "\n",
    "Chain consists of chardcoded list of steps, which one can later execute on custom input. \n",
    "Recap what we did in the previous notebook, first we formatted the prompt and then we send it to the model for processing.\n",
    "This is the simples example of a chain. Later we will explore more complex ones.\n",
    "\n",
    "\n",
    "### LangChain Expression Language (LCEL)\n",
    "\n",
    "LCEL is a declarative way to easily write and compose custom chains.\n",
    "Apart from enabling developers to do it in a very pythonic way, it has several other benefits.\n",
    "Any Chain constructed this way will automatically have full sync, async, batch, and streaming support. Also any components that can be run in parallel automatically are.\n",
    "\n",
    "Let's start with rewriting the chain from previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema.messages import HumanMessage\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a bear-themed joke for you:\n",
      "\n",
      "Why don't bears wear shoes?\n",
      "\n",
      "Because they have bear feet!\n"
     ]
    }
   ],
   "source": [
    "behaviour = \"an expert at telling jokes\"\n",
    "topic = \"bear\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"You are {behaviour}.\"),\n",
    "        HumanMessage(content=\"Hi. What's your name?\"),\n",
    "        AIMessagePromptTemplate.from_template(\"Hello. I am {behaviour}. What can I do for you today?\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_api = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0)\n",
    "\n",
    "# instead of\n",
    "# chat_resp = chat_api(chat_prompt.format_messages(behaviour=behaviour, topic=topic))\n",
    "# we can write\n",
    "chat_chain = chat_prompt | chat_api | StrOutputParser()\n",
    "\n",
    "res = chat_chain.invoke({\"behaviour\": behaviour, \"topic\": topic})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'behaviour': {'title': 'Behaviour', 'type': 'string'},\n",
       "  'topic': {'title': 'Topic', 'type': 'string'}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'StrOutputParserOutput', 'type': 'string'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.output_schema.schema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks\n",
    "\n",
    "As you can see, the code is cleaner and easy to understand.\n",
    "\n",
    "Now let's dig deeper and understand how those building blocks can be chained together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to remember, that the function used in chain must accept only one argument. \n",
    "So if you have a function which takes more than one argument, you can refactor it, to take a dictionary of inputs as argument or call it from another function which does so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import (\n",
    "    RunnableLambda, \n",
    "    RunnableMap, \n",
    "    RunnableBranch,\n",
    "    RunnablePassthrough\n",
    ")\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableLambda` is a wrapper around arbitrary function which makes it compatible with Expression Language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableLambda(lambda x: x + 1) | (lambda x: x * 7) | (lambda x: x - 1) # type: ignore\n",
    "chain.invoke(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 2 3 4'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_custom_fn(a, b, c, d):\n",
    "    return f\"{a} {b} {c} {d}\"\n",
    "\n",
    "def fn_wrapper(inputs):\n",
    "    a = inputs[\"a\"]\n",
    "    b = inputs[\"b\"]\n",
    "    c = inputs[\"c\"]\n",
    "    d = inputs[\"d\"]\n",
    "    return my_custom_fn(a, b, c, d)\n",
    "\n",
    "\n",
    "chain = RunnableLambda(fn_wrapper)\n",
    "chain.invoke({\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 2 3 4'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or using the inspect module\n",
    "from inspect import signature\n",
    "def fn_wrapper2(inputs):\n",
    "    fn_sig = signature(my_custom_fn)\n",
    "    return my_custom_fn(**{k: inputs[k] for k in fn_sig.parameters.keys()})\n",
    "\n",
    "chain = RunnableLambda(fn_wrapper2)\n",
    "chain.invoke({\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableMap` can execute multiple Runnables in parallel and return the output of these Runnables as a map.\n",
    "\n",
    "Each element of the map will get the same input and all the outputs will be combined into single dictionary.\n",
    "\n",
    "Note that the input is passed as in standard Python function call, so it can be modified by the function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 inputs: [1, 2, 3]\n",
      "f2 inputs: [1, 2, 3]\n",
      "\n",
      "\n",
      "f3 inputs: [1, 2, 3, 'f1']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1 return value': [1, 2, 3, 'f1'],\n",
       " 'f2 return value': [1, 2, 3, 'f2'],\n",
       " 'f3 return value': {'some_name': 'f3'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1(inputs):\n",
    "    print(f\"f1 inputs: {inputs}\\n\")\n",
    "    inputs.append('f1')\n",
    "    return inputs\n",
    "\n",
    "def f2(inputs):\n",
    "    print(f\"f2 inputs: {inputs}\\n\")\n",
    "    inputs_copy = inputs.copy()\n",
    "    inputs_copy.append('f2')\n",
    "    return inputs_copy\n",
    "\n",
    "def f3(inputs):\n",
    "    print(f\"f3 inputs: {inputs}\\n\")\n",
    "    return {\"some_name\": \"f3\"}\n",
    "\n",
    "chain = RunnableMap({\n",
    "    \"f1 return value\": f1,\n",
    "    \"f2 return value\": f2,\n",
    "    \"f3 return value\": f3\n",
    "})\n",
    "chain.invoke([1, 2, 3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `itemgetter` to extract arguments from input dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 2, 'f2': 'hello world'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def f1(x):\n",
    "    return x + 1\n",
    "\n",
    "def f2(y):\n",
    "    return y + \" world\"\n",
    "\n",
    "chain = RunnableMap({\n",
    "    \"f1\": itemgetter(\"x\") | RunnableLambda(f1),\n",
    "    \"f2\": itemgetter(\"y\") | RunnableLambda(f2),\n",
    "})\n",
    "\n",
    "chain.invoke({\"x\": 1, \"y\": \"hello\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableMap` runs in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0062379837036133\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from operator import itemgetter\n",
    "\n",
    "sleep_chain = RunnableMap({\n",
    "    't1': itemgetter('t1') | RunnableLambda(time.sleep), \n",
    "    't2': itemgetter('t2') | RunnableLambda(time.sleep),\n",
    "    't3': itemgetter('t3') | RunnableLambda(time.sleep),\n",
    "})\n",
    "\n",
    "start = time.time()\n",
    "sleep_chain.invoke({'t1': 3, 't2': 3, 't3': 3})\n",
    "end = time.time()   \n",
    "print(end - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is better to use explicit `Runnable` types when chaining with `|` operator\n",
    "\n",
    "You can chain runnable map with Python dictionary  using pipe `|` operator, but be carefull to do it on one dictionary, not few in a row.\n",
    "\n",
    "For example, this chain will not work as you might expect, insted the pipe operator will be interpreted as dict union operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'x': 'start first second last'}, {'x': 'start second last'})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type: ignore\n",
    "good_chain = RunnableMap({\n",
    "    'x': lambda inputs: inputs['x'] + 'first ',\n",
    "}) | RunnableMap({\n",
    "    'x': lambda inputs: inputs['x'] + 'second ',\n",
    "}) | RunnableMap({\n",
    "    'x': lambda inputs: inputs['x'] + 'last',\n",
    "}) \n",
    "\n",
    "bad_chain = {\n",
    "    'x': lambda inputs: inputs['x'] + 'first ',\n",
    "} | {\n",
    "    'x': lambda inputs: inputs['x'] + 'second ',\n",
    "} | RunnableMap({\n",
    "    'x': lambda inputs: inputs['x'] + 'last',\n",
    "}) \n",
    "\n",
    "good_chain.invoke({'x': 'start '}), bad_chain.invoke({'x': 'start '})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  x: RunnableLambda(...)\n",
       "}\n",
       "| {\n",
       "    x: RunnableLambda(...)\n",
       "  }\n",
       "| {\n",
       "    x: RunnableLambda(...)\n",
       "  }"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  x: RunnableLambda(...)\n",
       "}\n",
       "| {\n",
       "    x: RunnableLambda(...)\n",
       "  }"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnablePassthrough` passes the whole input further.\n",
    "\n",
    "It can be used to wrap a input into dictionary to provide keys necessary for some function, or make the input available in later chain steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"results for 'I am looking for ...': search results\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search(query):\n",
    "    return \"search results\"\n",
    "\n",
    "def combine_results(results):\n",
    "    query = results[\"query\"]\n",
    "    search_results = results[\"search_results\"]\n",
    "\n",
    "    return f\"results for '{query}': {search_results}\"\n",
    "\n",
    "chain = RunnableMap({\n",
    "    \"query\": RunnablePassthrough(),\n",
    "    \"search_results\": RunnableLambda(search),\n",
    "}) | RunnableLambda(combine_results)\n",
    "\n",
    "chain.invoke(\"I am looking for ...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableBranch` is used for conditional branching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handling int\n",
      "handling str\n",
      "handling default\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 'hello world', 1.0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def handle_int(x):\n",
    "    print(\"handling int\")\n",
    "    return x + 1\n",
    "\n",
    "def handle_str(x):\n",
    "    print(\"handling str\")\n",
    "    return x + \" world\"\n",
    "\n",
    "def default_handler(x):\n",
    "    print(\"handling default\")\n",
    "    return x\n",
    "\n",
    "chain = RunnableBranch(\n",
    "    (lambda x: isinstance(x, int), RunnableLambda(handle_int)),\n",
    "    (lambda x: isinstance(x, str), RunnableLambda(handle_str)),\n",
    "    default_handler\n",
    ")\n",
    "\n",
    "# or passing a function that handles routing\n",
    "def route(inputs):\n",
    "    if isinstance(inputs, int):\n",
    "        return handle_int(inputs)\n",
    "    elif isinstance(inputs, str):\n",
    "        return handle_str(inputs)\n",
    "    else:\n",
    "        return default_handler(inputs)\n",
    "    \n",
    "chain2 = RunnableLambda(route)\n",
    "\n",
    "chain.invoke(1), chain.invoke(\"hello\"), chain2.invoke(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing chat history with `ConversationBufferMemory`\n",
    "\n",
    "If `return_messages=True` it returns a list of messages in Human/AI format, otherwise it returns a string with predefined prefixes.\n",
    "Only one key per inputs and outputs is allowed by now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='What is 2+2?'), AIMessage(content='4')]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type: ignore\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context(\n",
    "    inputs={\n",
    "        'question': 'What is 2+2?',\n",
    "        # 'question2': 'What is 3+3?' # try uncommenting this line to see what happens\n",
    "    },\n",
    "    outputs={\n",
    "        'answer': '4'\n",
    "    }\n",
    ")\n",
    "memory.load_memory_variables(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my_custom_key': 'Human: What is 2+2?\\nBOT: 4\\nHuman: What is 2+2?\\nBOT: 4'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=False, memory_key='my_custom_key', ai_prefix='BOT')\n",
    "\n",
    "memory.save_context(\n",
    "    inputs={\n",
    "        'question1': 'What is 2+2?'\n",
    "    },\n",
    "    outputs={\n",
    "        'answer': '4'\n",
    "    }\n",
    ")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "##### Create a chain for chatting with the model and storing conversation history.\n",
    "\n",
    "There is already a `ConversationChain` implemented in LangChain but it puts everything into one Human prompt message and doesn't take advantage of System/Human/AI messages.\n",
    "\n",
    "You task is to create a chain which will use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: What is 2+2?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is 2+2?', 'history': '', 'response': '2+2 is equal to 4.'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type: ignore\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0),\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory(return_messages=False)\n",
    ")\n",
    "\n",
    "conversation.invoke(\"What is 2+2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What is 2+2?\n",
      "AI: 2+2 is equal to 4.\n",
      "Human: What was the result? I forgot.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What was the result? I forgot.',\n",
       " 'history': 'Human: What is 2+2?\\nAI: 2+2 is equal to 4.',\n",
       " 'response': 'The result of 2+2 is 4.'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(\"What was the result? I forgot.\") # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type: ignore\n",
    "from typing import Dict\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder # Prompt template that assumes variable is already list of messages.\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful chatbot.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "])\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"history\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "def get_question_and_history(input_string: str) -> Dict[str, str]:\n",
    "\n",
    "    return {\n",
    "        \"question\": ...,\n",
    "        \"history\": memory... # get list of history messages from memory\n",
    "    }\n",
    "\n",
    "def get_answer_and_pass_question_through(input_dict: Dict[str, str]) -> Dict[str, str]:\n",
    "\n",
    "    get_answer_chain = ... # format prompt > call model > extract output from AI Message object\n",
    "\n",
    "    return {\n",
    "        \"answer\": get_answer_chain.invoke(...), # recall from earlier, that prompt accepts a dictionary with key names == template variables\n",
    "        \"question\": ..., # pass question further \n",
    "    }\n",
    "\n",
    "def update_history_and_return_answer(inputs: Dict[str, str]) -> str:\n",
    "    # we use question/answer keys because they are required by the object, but it doesn't matter\n",
    "    memory.save_context({'question': ...}, {'answer': ...}) # store both question and answer in memory object, \n",
    "    \n",
    "    return ... # return answer string\n",
    "\n",
    "chat_chain = RunnableLambda(get_question_and_history) | RunnableLambda(get_answer_and_pass_question_through) | RunnableLambda(update_history_and_return_answer) | StrOutputParser()\n",
    "\n",
    "\n",
    "print(chat_chain.invoke(\"Hi. What's your name?\"))\n",
    "print(chat_chain.invoke(\"What is 2+2?\"))\n",
    "\n",
    "# see list of history messages\n",
    "print(...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am a chatbot, so I don't have a personal name. You can simply refer to me as \"chatbot\" or \"assistant.\" How can I assist you today?\n",
      "The sum of 2 and 2 is 4.\n",
      "[HumanMessage(content=\"Hi. What's your name?\"), AIMessage(content='Hello! I am a chatbot, so I don\\'t have a personal name. You can simply refer to me as \"chatbot\" or \"assistant.\" How can I assist you today?'), HumanMessage(content='What is 2+2?'), AIMessage(content='The sum of 2 and 2 is 4.')]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful chatbot.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "])\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"history\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "\n",
    "def get_question_and_history(input_str: str) -> Dict[str, str]:\n",
    "    return {\n",
    "        \"question\": input_str,\n",
    "        \"history\": memory.load_memory_variables(inputs=None)['history'], # type: ignore\n",
    "    }\n",
    "\n",
    "def get_answer_and_pass_question_through(input_dict: Dict[str, str]) -> Dict[str, str]:\n",
    "    get_answer_chain = prompt | model | StrOutputParser()\n",
    "    return {\n",
    "        \"question\": input_dict['question'],\n",
    "        \"answer\": get_answer_chain.invoke(input_dict),\n",
    "    }\n",
    "\n",
    "def update_history_and_return_answer(inputs: Dict[str, str]) -> str:\n",
    "    memory.save_context({'question': inputs['question']}, {'answer': inputs['answer']})\n",
    "\n",
    "    return inputs['answer']\n",
    "\n",
    "chat_chain = RunnableLambda(get_question_and_history) | RunnableLambda(get_answer_and_pass_question_through) | RunnableLambda(update_history_and_return_answer)\n",
    "\n",
    "print(chat_chain.invoke(\"Hi. What's your name?\"))\n",
    "print(chat_chain.invoke(\"What is 2+2?\"))\n",
    "\n",
    "print(memory.load_memory_variables({})['history'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
