{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Chains\n",
    "\n",
    "In this notebook we will go through the most popular usecase for LangChain which is building Chains.\n",
    "\n",
    "### What is a Chain?\n",
    "\n",
    "Chain consists of chardcoded list of steps, which one can later execute on custom input. \n",
    "Recap what we did in the previous notebook, first we formatted the prompt and then we send it to the model for processing.\n",
    "This is the simples example of a chain. Later we will explore more complex ones.\n",
    "\n",
    "\n",
    "### LangChain Expression Language (LCEL)\n",
    "\n",
    "LCEL is a declarative way to easily write and compose custom chains.\n",
    "Apart from enabling developers to do it in a very pythonic way, it has several other benefits.\n",
    "Any Chain constructed this way will automatically have full sync, async, batch, and streaming support. Also any components that can be run in parallel automatically are.\n",
    "\n",
    "Let's start with rewriting the chain from previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    ")\n",
    "from langchain.prompts.chat import HumanMessage\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a bear-themed joke for you:\n",
      "\n",
      "Why don't bears wear shoes?\n",
      "\n",
      "Because they have bear feet!\n"
     ]
    }
   ],
   "source": [
    "behaviour = \"an expert at telling jokes\"\n",
    "topic = \"bear\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"You are {behaviour}.\"),\n",
    "        HumanMessage(content=\"Hi. What's your name?\"),\n",
    "        AIMessagePromptTemplate.from_template(\"Hello. I am {behaviour}. What can I do for you today?\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_api = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0)\n",
    "\n",
    "chat_chain = chat_prompt | chat_api | StrOutputParser()\n",
    "\n",
    "res = chat_chain.invoke({\"behaviour\": behaviour, \"topic\": topic})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'behaviour': {'title': 'Behaviour'},\n",
       "  'topic': {'title': 'Topic'}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'StrOutputParserOutput', 'type': 'string'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.output_schema.schema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks\n",
    "\n",
    "As you can see, the code is cleaner and easy to understand.\n",
    "\n",
    "Now let's dig deeper and understand how those building blocks can be chained together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import (\n",
    "    RunnableLambda, \n",
    "    RunnableMap, \n",
    "    RunnableBranch,\n",
    "    RunnablePassthrough\n",
    ")\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from operator import itemgetter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableLambda` is a wrapper arounf arbitrary function which makes it compatible with Expression Language.\n",
    "\n",
    "It is important to remember, that the function used in chain must accept only one argument. \n",
    "So if you have a function which takes more than one argument, you can refactor it, to take a dictionary of inputs as argument or call it from another function which does so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableLambda(lambda x: x + 1) | (lambda x: x * 2) # this syntax also works\n",
    "chain.invoke(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 2 3 4'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_custom_fn(a, b, c, d):\n",
    "    return f\"{a} {b} {c} {d}\"\n",
    "\n",
    "def fn_wrapper(inputs):\n",
    "    a = inputs[\"a\"]\n",
    "    b = inputs[\"b\"]\n",
    "    c = inputs[\"c\"]\n",
    "    d = inputs[\"d\"]\n",
    "    return my_custom_fn(a, b, c, d)\n",
    "\n",
    "# or using the inspect module\n",
    "from inspect import signature\n",
    "def fn_wrapper2(inputs):\n",
    "    fn_sig = signature(my_custom_fn)\n",
    "    return my_custom_fn(**{k: inputs[k] for k in fn_sig.parameters.keys()})\n",
    "\n",
    "chain = RunnableLambda(fn_wrapper)\n",
    "chain.invoke({\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableMap` can execute multiple Runnables in parallel and return the output of these Runnables as a map.\n",
    "\n",
    "Each element of the map will get a full copy of the input and all the outputs will get combined into single dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 inputs: [1, 2, 3]\n",
      "f2 inputs: [1, 2, 3]\n",
      "f3 inputs: [1, 2, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1 return value': 'f1',\n",
       " 'f2 return value': ['f2', 'f2'],\n",
       " 'f3 return value': {'some_name': 'f3'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1(inputs):\n",
    "    print(f\"f1 inputs: {inputs}\")\n",
    "    return \"f1\"\n",
    "\n",
    "def f2(inputs):\n",
    "    print(f\"f2 inputs: {inputs}\")\n",
    "    return [\"f2\", \"f2\"]\n",
    "\n",
    "def f3(inputs):\n",
    "    print(f\"f3 inputs: {inputs}\")\n",
    "    return {\"some_name\": \"f3\"}\n",
    "\n",
    "chain = RunnableMap({\n",
    "    \"f1 return value\": f1,\n",
    "    \"f2 return value\": f2,\n",
    "    \"f3 return value\": f3\n",
    "})\n",
    "chain.invoke([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 inputs: [1, 2, 3]\n",
      "f2 inputs: [1, 2, 3]\n",
      "f3 inputs: [1, 2, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'copy nr. 1': {'f1 return value': 'f1',\n",
       "  'f2 return value': ['f2', 'f2'],\n",
       "  'f3 return value': {'some_name': 'f3'}},\n",
       " 'copy nr. 2': {'f1 return value': 'f1',\n",
       "  'f2 return value': ['f2', 'f2'],\n",
       "  'f3 return value': {'some_name': 'f3'}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2 = chain | {\n",
    "    \"copy nr. 1\": lambda x: x,\n",
    "    \"copy nr. 2\": lambda x: x,\n",
    "}\n",
    "\n",
    "chain2.invoke([1, 2, 3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `itemgetter` to extract arguments from input dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 2, 'f2': 'hello world'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1(x):\n",
    "    return x + 1\n",
    "\n",
    "def f2(y):\n",
    "    return y + \" world\"\n",
    "\n",
    "chain = RunnableMap({\n",
    "    \"f1\": itemgetter(\"x\") | RunnableLambda(f1),\n",
    "    \"f2\": itemgetter(\"y\") | RunnableLambda(f2),\n",
    "})\n",
    "\n",
    "chain.invoke({\"x\": 1, \"y\": \"hello\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableMap` runs in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.006687879562378\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from operator import itemgetter\n",
    "\n",
    "sleep_chain = RunnableMap({\n",
    "    't1': itemgetter('t1') | RunnableLambda(time.sleep), \n",
    "    't2': itemgetter('t2') | RunnableLambda(time.sleep),\n",
    "    't3': itemgetter('t3') | RunnableLambda(time.sleep),\n",
    "})\n",
    "\n",
    "start = time.time()\n",
    "sleep_chain.invoke({'t1': 3, 't2': 3, 't3': 3})\n",
    "end = time.time()   \n",
    "print(end - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to write runnable maps as Python dictionaries and chain them using pipe `|` operator, there must be at leas one instance of Runnable class in the chain.\n",
    "For example, this chain will not work, insted the pipe operator will be interpreted as dict union operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'some_fn': RunnableLambda(...), 'some_other_fn': RunnableLambda(...), 'yet_another_fn': RunnableLambda(...)}\n"
     ]
    }
   ],
   "source": [
    "my_failed_chain = {\n",
    "    'some_fn': RunnableLambda(lambda x: x + 1),\n",
    "} | {\n",
    "    'some_other_fn': RunnableLambda(lambda x: x + 1),\n",
    "} | {\n",
    "    'yet_another_fn': RunnableLambda(lambda x: x + 1),\n",
    "}\n",
    "\n",
    "print(type(my_failed_chain))\n",
    "print(my_failed_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain.schema.runnable.base.RunnableSequence'>\n",
      "first={\n",
      "  some_fn: RunnableLambda(...),\n",
      "  some_other_fn: RunnableLambda(...)\n",
      "} last={\n",
      "  yet_another_fn: RunnableLambda(...)\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "my_working_chain = {\n",
    "    'some_fn': RunnableLambda(lambda x: x + 1),\n",
    "} | {\n",
    "    'some_other_fn': RunnableLambda(lambda x: x + 1),\n",
    "} | RunnableMap({\n",
    "    'yet_another_fn': RunnableLambda(lambda x: x + 1),\n",
    "})\n",
    "\n",
    "print(type(my_working_chain))\n",
    "print(my_working_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnablePassthrough` passes the whole input further.\n",
    "\n",
    "It can be used to wrap a input into dictionary to provide keys necessary for some function, or make the input available in later chain steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"results for 'I am looking for ...': search results\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search(query):\n",
    "    return \"search results\"\n",
    "\n",
    "def combine_results(results):\n",
    "    query = results[\"query\"]\n",
    "    search_results = results[\"search_results\"]\n",
    "\n",
    "    return f\"results for '{query}': {search_results}\"\n",
    "\n",
    "chain = RunnableMap({\n",
    "    \"query\": RunnablePassthrough(),\n",
    "    \"search_results\": RunnableLambda(search),\n",
    "}) | RunnableLambda(combine_results)\n",
    "\n",
    "chain.invoke(\"I am looking for ...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableBranch` is used for conditional branching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handling int\n",
      "handling str\n",
      "handling default\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 'hello world', 1.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def handle_int(x):\n",
    "    print(\"handling int\")\n",
    "    return x + 1\n",
    "\n",
    "def handle_str(x):\n",
    "    print(\"handling str\")\n",
    "    return x + \" world\"\n",
    "\n",
    "def default_handler(x):\n",
    "    print(\"handling default\")\n",
    "    return x\n",
    "\n",
    "chain = RunnableBranch(\n",
    "    (lambda x: isinstance(x, int), RunnableLambda(handle_int)),\n",
    "    (lambda x: isinstance(x, str), RunnableLambda(handle_str)),\n",
    "    default_handler\n",
    ")\n",
    "\n",
    "# or passing a function that handles routing\n",
    "def route(inputs):\n",
    "    if isinstance(inputs, int):\n",
    "        return handle_int(inputs)\n",
    "    elif isinstance(inputs, str):\n",
    "        return handle_str(inputs)\n",
    "    else:\n",
    "        return default_handler(inputs)\n",
    "    \n",
    "chain2 = RunnableLambda(route)\n",
    "\n",
    "chain.invoke(1), chain.invoke(\"hello\"), chain2.invoke(1.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "##### Create a chain for chatting with the model and storing conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'invoke'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 41\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m \u001b[39m# return answer string\u001b[39;00m\n\u001b[1;32m     38\u001b[0m chat_chain \u001b[39m=\u001b[39m RunnableLambda(get_question_and_history) \u001b[39m|\u001b[39m RunnableLambda(get_answer_and_pass_question_through) \u001b[39m|\u001b[39m RunnableLambda(update_history_and_return_answer) \u001b[39m|\u001b[39m StrOutputParser()\n\u001b[0;32m---> 41\u001b[0m \u001b[39mprint\u001b[39m(chat_chain\u001b[39m.\u001b[39;49minvoke(\u001b[39m\"\u001b[39;49m\u001b[39mHi. What\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms your name?\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     42\u001b[0m \u001b[39mprint\u001b[39m(chat_chain\u001b[39m.\u001b[39minvoke(\u001b[39m\"\u001b[39m\u001b[39mWhat is 2+2?\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     44\u001b[0m \u001b[39m# see list of history messages\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PROJECTS/workshops/LLM_intro/env/lib/python3.10/site-packages/langchain/schema/runnable/base.py:1372\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1371\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 1372\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   1373\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   1374\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1375\u001b[0m             patch_config(\n\u001b[1;32m   1376\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1377\u001b[0m             ),\n\u001b[1;32m   1378\u001b[0m         )\n\u001b[1;32m   1379\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/PROJECTS/workshops/LLM_intro/env/lib/python3.10/site-packages/langchain/schema/runnable/base.py:2348\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m   2342\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   2343\u001b[0m     \u001b[39minput\u001b[39m: Input,\n\u001b[1;32m   2344\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2345\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   2346\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Output:\n\u001b[1;32m   2347\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfunc\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 2348\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[1;32m   2349\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_invoke,\n\u001b[1;32m   2350\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   2351\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_config(config, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc),\n\u001b[1;32m   2352\u001b[0m         )\n\u001b[1;32m   2353\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2354\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   2355\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2356\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUse `ainvoke` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2357\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/PROJECTS/workshops/LLM_intro/env/lib/python3.10/site-packages/langchain/schema/runnable/base.py:472\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    466\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    467\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[1;32m    468\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[1;32m    469\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    470\u001b[0m )\n\u001b[1;32m    471\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 472\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[1;32m    473\u001b[0m         func, \u001b[39minput\u001b[39;49m, run_manager, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    474\u001b[0m     )\n\u001b[1;32m    475\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    476\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Desktop/PROJECTS/workshops/LLM_intro/env/lib/python3.10/site-packages/langchain/schema/runnable/config.py:162\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mif\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    161\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/PROJECTS/workshops/LLM_intro/env/lib/python3.10/site-packages/langchain/schema/runnable/base.py:2282\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config)\u001b[0m\n\u001b[1;32m   2276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_invoke\u001b[39m(\n\u001b[1;32m   2277\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   2278\u001b[0m     \u001b[39minput\u001b[39m: Input,\n\u001b[1;32m   2279\u001b[0m     run_manager: CallbackManagerForChainRun,\n\u001b[1;32m   2280\u001b[0m     config: RunnableConfig,\n\u001b[1;32m   2281\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Output:\n\u001b[0;32m-> 2282\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, \u001b[39minput\u001b[39;49m, run_manager, config)\n\u001b[1;32m   2283\u001b[0m     \u001b[39m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   2284\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/Desktop/PROJECTS/workshops/LLM_intro/env/lib/python3.10/site-packages/langchain/schema/runnable/config.py:162\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mif\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    161\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[17], line 28\u001b[0m, in \u001b[0;36mget_answer_and_pass_question_through\u001b[0;34m(input_dict)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_answer_and_pass_question_through\u001b[39m(input_dict: Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m     25\u001b[0m     get_answer_chain \u001b[39m=\u001b[39m \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m \u001b[39m# format prompt > call model > extract output from AI Message object\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m---> 28\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m: get_answer_chain\u001b[39m.\u001b[39;49minvoke(\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m), \u001b[39m# recall from earlier, that prompt accepts a dictionary with key names == template variables\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m# pass question further \u001b[39;00m\n\u001b[1;32m     30\u001b[0m     }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'invoke'"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful chatbot.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "])\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"history\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "def get_question_and_history(input_string: str) -> Dict[str, str]:\n",
    "\n",
    "    return {\n",
    "        \"question\": ...,\n",
    "        \"history\": memory.load_memory_variables(inputs=None) # get list of history messages from memory\n",
    "    }\n",
    "\n",
    "def get_answer_and_pass_question_through(input_dict: Dict[str, str]) -> Dict[str, str]:\n",
    "\n",
    "    get_answer_chain = ... # format prompt > call model > extract output from AI Message object\n",
    "\n",
    "    return {\n",
    "        \"answer\": get_answer_chain.invoke(...), # recall from earlier, that prompt accepts a dictionary with key names == template variables\n",
    "        \"question\": ..., # pass question further \n",
    "    }\n",
    "\n",
    "def update_history_and_return_answer(inputs: Dict[str, str]) -> str:\n",
    "    # we use question/answer keys because they are required by the object, but we dont use it here\n",
    "    memory.save_context({'question': ...}, {'answer': ...}) # store both question and answer in memory object, \n",
    "    \n",
    "    return ... # return answer string\n",
    "\n",
    "chat_chain = RunnableLambda(get_question_and_history) | RunnableLambda(get_answer_and_pass_question_through) | RunnableLambda(update_history_and_return_answer) | StrOutputParser()\n",
    "\n",
    "\n",
    "print(chat_chain.invoke(\"Hi. What's your name?\"))\n",
    "print(chat_chain.invoke(\"What is 2+2?\"))\n",
    "\n",
    "# see list of history messages\n",
    "print(...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am a chatbot, so I don't have a personal name. You can simply refer to me as \"Chatbot\" or any other name you prefer. How can I assist you today?\n",
      "The sum of 2 and 2 is 4.\n",
      "[HumanMessage(content=\"Hi. What's your name?\"), AIMessage(content='Hello! I am a chatbot, so I don\\'t have a personal name. You can simply refer to me as \"Chatbot\" or any other name you prefer. How can I assist you today?'), HumanMessage(content='What is 2+2?'), AIMessage(content='The sum of 2 and 2 is 4.')]\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful chatbot.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "])\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"history\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "\n",
    "def get_question_and_history(input_str: str) -> Dict[str, str]:\n",
    "    return {\n",
    "        \"question\": input_str,\n",
    "        \"history\": memory.load_memory_variables(inputs=None)['history'],\n",
    "    }\n",
    "\n",
    "def get_answer_and_pass_question_through(input_dict: Dict[str, str]) -> Dict[str, str]:\n",
    "    get_answer_chain = prompt | model | StrOutputParser()\n",
    "    return {\n",
    "        \"question\": input_dict['question'],\n",
    "        \"answer\": get_answer_chain.invoke(input_dict),\n",
    "    }\n",
    "\n",
    "def update_history_and_return_answer(inputs: Dict[str, str]) -> str:\n",
    "    memory.save_context({'question': inputs['question']}, {'answer': inputs['answer']})\n",
    "\n",
    "    return inputs['answer']\n",
    "\n",
    "chat_chain = RunnableLambda(get_question_and_history) | RunnableLambda(get_answer_and_pass_question_through) | RunnableLambda(update_history_and_return_answer)\n",
    "\n",
    "print(chat_chain.invoke(\"Hi. What's your name?\"))\n",
    "print(chat_chain.invoke(\"What is 2+2?\"))\n",
    "\n",
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
