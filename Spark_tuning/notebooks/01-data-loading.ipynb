{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80dff125-7b12-4875-bcf0-78af4097d2f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data loading\n",
    "\n",
    "Our pipeline starts with data loading step. Each batch run input files are delivered on location of `/Volumes/tantusdata_playground/default/bde-2023/input-partitioning/products-4096.parquet/`. The usual read & write time od 11s is considered too high, we need to lower it by a magniture of 2 or 3. \n",
    "\n",
    "## The goal\n",
    "We want to decrease computing time in this notebook. Look into the UI and try to validate the performance problem. Then design and test a solution that modifies the input date _without changing any of its contents_ (so: pre-computing anything is not allowed).\n",
    "\n",
    "The goal is not \"5s is less than 7s\", but to understand _why_ this change helped and how it can be fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f0dd7a0-ec65-4458-9691-8f37d65dad12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Input data path - this is the data that you need to load.\n",
    "INPUT_PATH = \"/Volumes/tantusdata_playground/default/bde-2023/input-partitioning/products-4096.parquet/\"\n",
    "\n",
    "# Volume path - make sure you have created a volume for yourself.\n",
    "VOLUME_PATH = \"/Volumes/tantusdata_playground/default/test-user-001\"\n",
    "\n",
    "IMMEDIATE_PATH = f\"{VOLUME_PATH}/01-data-loading/staging/products.parquet\"\n",
    "WRITE_PATH = f\"{VOLUME_PATH}/01-data-loading/products.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e84346ac-7a59-4a66-8bd9-3faaed7d3ab6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Your own code:\n",
    "Since we cannot interfere with the input, we are to emulate changing the partitioning there. **Write a step that**:\n",
    "1. Loads the data from `INPUT_PATH`. \n",
    "2. Reorganizes the loaded data _without changing the contents_ (no pre-computing aggregates or anything similar). Try repartitioning the data.\n",
    "3. Writes the data to `IMMEDIATE_PATH`. \n",
    "\n",
    "Afterwards the aggregation job should take less to complete. Try experimenting with different partitioning settings. You can use assertions at the end of the notebook to check job validity, but not the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b496ce7-0544-43d8-b417-d055a8784a3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FixMe: remove the line below and put your own processing step (job) here.\n",
    "IMMEDIATE_PATH = INPUT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53369063-6efb-4be9-9833-cff5bee7ff3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## The aggregation job\n",
    "This job should complete faster solely due to your previous optimizations - do not modify it, rerun as-is to check the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c3429f-d2ee-41da-8647-78e5a6b3c514",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_input = spark.read.parquet(IMMEDIATE_PATH)\n",
    "df_transformed = (df_input\n",
    "    .groupBy(\"unit\")\n",
    "    .agg(\n",
    "        F.count_distinct(\"productID\").alias(\"productsCount\"), \n",
    "        F.avg(\"price\").alias(\"averagePrice\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_transformed.write.mode(\"overwrite\").parquet(WRITE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed04031b-f32d-40e8-b5e7-790daa1852a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ddf04a3-d309-40c5-b34c-991340703b65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert(spark.read.parquet(IMMEDIATE_PATH).count() == 500_000)\n",
    "assert(spark.read.parquet(WRITE_PATH).count() == 5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-data-loading",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
