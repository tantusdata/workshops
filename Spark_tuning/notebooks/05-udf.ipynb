{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cff41a5a-120f-405b-9272-db8698c4bdb7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# UDF\n",
    "Using UDF is one of the \"classic\" optimization guideline. Indeed using UDF (or worse - using non-arrow python UDF) addds _linear_ overhead to the application and may interfere with Catalyst optimizer. In this example we will see it in action\n",
    "\n",
    "## The goal\n",
    "The current (naive) implementation uses three UDF - they are not overly complicated and should be easy to translate to native Spark function calls. Try doing so and run _both_ functions. What difference can you see in e.g. physical execution plan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aebd016-8166-43a1-b8ef-dc96310058e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import random\n",
    "\n",
    "INPUT_PATH = \"/Volumes/tantusdata_playground/default/bde-2023/input/orders.parquet/\"\n",
    "\n",
    "# Volume path - make sure you have created a volume for yourself.\n",
    "VOLUME_PATH = \"/Volumes/tantusdata_playground/default/admin-user\"\n",
    "\n",
    "WRITE_PATH = f\"{VOLUME_PATH}/03-cross-join/orders-pairs.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f12170-ba3f-4e91-9b2a-ef3dde45cce6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders = spark.read.parquet(INPUT_PATH)\n",
    "\n",
    "udf_1 = udf(lambda orderID: random.randint(1, 100) + orderID, \"BIGINT\")\n",
    "udf_2 = udf(lambda orderID: orderID == 0, \"BOOLEAN\")\n",
    "udf_3 = udf(lambda storeID: \"invalid\" if int(storeID) < 0 else str(storeID), \"STRING\")\n",
    "            \n",
    "orders_with_extras = (orders\n",
    "    .withColumn(\"extra_1\", udf_1(\"orderID\"))\n",
    "    .withColumn(\"extra_2\", udf_2(\"orderID\"))\n",
    "    .withColumn(\"extra_3\", udf_3(\"storeID\"))\n",
    "    .filter(F.col(\"extra_2\") == True)\n",
    ")\n",
    "\n",
    "orders_with_extras.write.parquet(WRITE_PATH, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e45c531-f12a-44bd-8930-1c19e10bbf17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders = spark.read.parquet(INPUT_PATH)\n",
    "\n",
    "# FixMe: try to replace UDFs with native functions, such as randn or when.\n",
    "\n",
    "orders_with_extras = (orders\n",
    "    .withColumn(\"extra_1\", )\n",
    "    .withColumn(\"extra_2\", )\n",
    "    .withColumn(\"extra_3\", )\n",
    "    .filter(F.col(\"extra_2\") == True)\n",
    ")\n",
    "\n",
    "orders_with_extras.write.parquet(WRITE_PATH, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05-udf",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
