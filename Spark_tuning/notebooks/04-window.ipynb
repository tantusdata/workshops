{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ae05dee-b00c-48b8-a9b1-84002e016a2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Window function\n",
    "Window is one of the grouping operations, it is also a bit trickier to salt if the groups are skewed. There is a way to fix skew efficiently, though. We will look into it in this notebook.\n",
    "\n",
    "In this example we are using events table. This query assumes that for each row we need data of the next row - assuming all rows are sorted by event date. To use that we do a window function - window of `[row, row + 1]` and we calculate `LEAD(event_date)`.\n",
    "\n",
    "## The goal\n",
    "The current implementation needs tweaking. You are welcome to experiment and try any of your ideas. However, if you need an inspiration, try going in this plan:\n",
    "1. Bucket the input data into N buckets. You can use `F.unix_timestamp` to parse the `event_date` column.\n",
    "2. Run the windowing function. You will (at least - should) see gaps within the windows on the edges of the bucket.\n",
    "3. Identify the problematic rows and address them separately. You will have N times less data than in the full dataset.\n",
    "\n",
    "## Mending the holes\n",
    "1. For each window pick rows from the border (edge cases, literally) - the first one and the last one (per bucket).\n",
    "2. Recompute `LEAD(event_date)` for each of those cases, without bucketing. This way event at the end of each bucket will `see` first event of other bucket, thus will find the correct date instead of `NULL`.\n",
    "3. With last row (per bucket) correctly computed, purge first rows - now they have incorrect lead time. It does not matter, the bucketed dataset already has them correct.\n",
    "4. Merge the main dataset with fixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93bd74b-51ee-494b-93e1-e36c8a0581d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# We are using events dataset, 1B of events associated with 100k users, skewed.\n",
    "INPUT_PATH = \"/Volumes/tantusdata_playground/default/bde-2023/input/events/events.parquet\"\n",
    "\n",
    "# Volume path - make sure you have created a volume for yourself.\n",
    "VOLUME_PATH = \"/Volumes/tantusdata_playground/default/test-user-001\"\n",
    "\n",
    "# Pick any path you like within your volume.\n",
    "WRITE_PATH = f\"{VOLUME_PATH}/04-window/events.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2210d42d-8156-4b10-ad5c-8a5e7ddace3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "events = spark.read.parquet(INPUT_PATH)\n",
    "\n",
    "window = Window.partitionBy(\"userID\").orderBy(\"eventTimestamp\")\n",
    "windowed_events = events.withColumn(\"nextEvent\", F.lead(\"eventTimestamp\", 1).over(window))\n",
    "\n",
    "windowed_events.write.parquet(WRITE_PATH)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04-window",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
