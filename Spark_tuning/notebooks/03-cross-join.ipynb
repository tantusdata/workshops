{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "189bb6f6-6373-44fb-9e6e-08252bdd34e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Cross Join\n",
    "Cross join is always an expensive operation, mostly due to the amount of data it produces. However even even here there are ways to do it efficiently and ways to keep waiting long hours for a job to complete. In this notebook we will look at a job that computes cross-join of orders, grouped by order ID. We want to produce all product pairs per every order available.\n",
    "\n",
    "## The goal\n",
    "The current (naive) implementation takes over 1 hour to compute all the pairs - you can verify this by running the current implementation. Browse through the data and see what is causing the problem, then try to correct this. A really optimized solution may take as little as 10 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d5bcd81-4007-4b44-9125-74fb60b785f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "INPUT_PATH = \"/FileStore/input/orders.parquet/\"\n",
    "\n",
    "# Volume path - make sure you have created a volume for yourself.\n",
    "VOLUME_PATH = \"/Volumes/tantusdata_playground/default/test-user-001\"\n",
    "\n",
    "WRITE_PATH = f\"{VOLUME_PATH}/03-cross-join/orders-pairs.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c03c6b0-0df4-4af1-a7f9-a69cb427dcdc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Your own code:\n",
    "1. Do not change input nor output paths.\n",
    "2. The aim is to optimize the join. \n",
    "3. Non-destructable changes (adding columns, partitions, hints) are all welcomed. You may need to change both sides of the join.\n",
    "4. The default implementation is very slow - takes over 1h to complete. Try to identify the issue - browsing through the data may help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70e50890-a9d8-42f1-8962-202d63099210",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders = spark.read.parquet(INPUT_PATH)\n",
    "\n",
    "orders_lhs = orders.select(\n",
    "    F.col(\"orderID1\"), \n",
    "    F.col(\"productID\").alias(f\"productID1\")\n",
    ")\n",
    "\n",
    "orders_rhs = orders.select(\n",
    "    F.col(\"orderID2\"), \n",
    "    F.col(\"productID\").alias(f\"productID1\")\n",
    ")\n",
    "\n",
    "# FixMe: try to optimize this join. Make sure to check the data being processed by this job.\n",
    "\n",
    "orders_pairs = (orders_lhs\n",
    "    .join(orders_rhs, F.col(\"orderID1\") == F.col(\"orderID2\")) & (F.col(\"productID1\") < F.col(\"productID2\"))\n",
    ")\n",
    "\n",
    "orders_pairs.write.parquet(WRITE_PATH)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03-cross-join",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
