{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eaf0a5f-a04f-4e07-96c0-f54341628274",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Refactoring\n",
    "\n",
    "Sometimes the Spark jobs are a subject for refactoring. Instead of a simple read - process - write script in Scala / Python they become an integration work of multiple commons and libraries. In this cases it is important to keep the actual logic afloat instead of bury it under the function calls.\n",
    "\n",
    "## Our example\n",
    "We have an example of a over-engineered function that loads events data. It is used to load the data and register it as a temporary view. Sadly, it does a bit of unnecessary computations along - look at the actual job (the SQL query) that is being run against this data.\n",
    "\n",
    "## The goal\n",
    "No actual goal - this is just an ilustrative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a75daf-f41e-4f71-b90b-a81b02f4dd78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "INPUT_PATH = \"/Volumes/tantusdata_playground/default/bde-2023/input/events/events.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cc057a5-ad2a-4bad-a9d6-a325ee1db5d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_events():\n",
    "    window = Window.partitionBy(\"userID\").orderBy(\"eventTimestamp\")\n",
    "\n",
    "    (spark.read.parquet(INPUT_PATH)\n",
    "        .withColumn(\"date_ranking\", F.rank().over(window))\n",
    "        .cache()\n",
    "        .createOrReplaceTempView(\"refactoring_example\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad684924-bb4c-45dd-91c4-18524f5d924a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_events()\n",
    "display(spark.sql(\"SELECT * FROM refactoring_example ORDER BY eventTimestamp DESC LIMIT 1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0061273-fafb-42a8-b17f-4a990b58af0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.parquet(INPUT_PATH)\n",
    "    .orderBy(F.col(\"eventTimestamp\").desc())\n",
    "    .limit(1)        \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06-refactoring",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
